@tool("source_credibility_checker", return_direct=True)
def source_credibility_checker(source_url: str) -> str:
    """
    Evaluates the credibility of a given source using LLM reasoning.

    The model analyzes:
      - Domain authority (.gov, .edu, .org, etc.)
      - Potential bias or sensationalism in domain name
      - Known institution or journal reputation
      - Contextual reliability cues (research org, media outlet)
    
    Returns:
        str: Natural-language explanation and credibility score (0-1)
    """
    url = source_url.lower()

    # Base heuristic for domain
    domain_score = 0.5
    if re.search(r"\.gov|\.edu|\.ac\.", url):
        domain_score += 0.4
    elif re.search(r"\.org", url):
        domain_score += 0.2
    elif re.search(r"blogspot|substack|wordpress|medium", url):
        domain_score -= 0.2

    # Ask LLM for reasoning-based credibility evaluation
    reasoning = llm.invoke(f"""
    Evaluate the credibility of this source: {source_url}
    Criteria:
    1. Is it from an official or academic organization?
    2. Does it likely contain verified, peer-reviewed, or expert content?
    3. Is there evidence of bias or low editorial standards?
    Give a reliability label (High/Moderate/Low) and justify briefly.
    """).content

    # Estimate score from reasoning text
    if "high" in reasoning.lower():
        llm_score = 0.9
    elif "moderate" in reasoning.lower():
        llm_score = 0.7
    else:
        llm_score = 0.4

    final_score = (domain_score + llm_score) / 2
    label = "High" if final_score >= 0.8 else "Moderate" if final_score >= 0.6 else "Low"

    return (
        f"🔍 Source: {source_url}\n"
        f"Credibility: {label}\n"
        f"Score: {final_score:.2f}\n\n"
        f"LLM Assessment:\n{reasoning.strip()}"
    )
