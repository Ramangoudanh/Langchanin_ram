from langchain.agents import initialize_agent, AgentType
from langchain.chat_models import ChatOpenAI
from langgraph.middleware import SummarizationMiddleware

# your LLM and tools
llm = ChatOpenAI(model="gpt-4o")
tools = []  # your tools list

# custom + prebuilt middleware
middlewares = [
    CustomLoggerMiddleware(),
    SummarizationMiddleware(max_summary_tokens=200)
]

agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    middleware=middlewares,   # 👈 pass list here
    verbose=True
)

# 🧩 Imports
from langchain.callbacks.base import BaseCallbackHandler

# ✅ Define the terminal logger
class TerminalLogger(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        print("\n🤖 LLM START:")
        for p in prompts:
            print(p)

    def on_tool_start(self, serialized, input_str, **kwargs):
        print(f"\n🧰 TOOL USED: {serialized.get('name')}")
        print(f"➡️ INPUT: {input_str}")

    def on_tool_end(self, output, **kwargs):
        print(f"✅ TOOL OUTPUT: {output}")

    def on_llm_end(self, response, **kwargs):
        # Safely extract token usage if available
        usage = {}
        if hasattr(response, "llm_output") and response.llm_output:
            usage = response.llm_output.get("token_usage", {})
        print(f"📊 TOKEN USAGE: {usage}")
